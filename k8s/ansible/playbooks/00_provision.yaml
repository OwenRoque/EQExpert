---
- name: Provision AWS infra for Kubernetes Cluster
  hosts: localhost
  connection: local
  vars:
    aws_region: "us-east-1"
    vpc_cidr: "10.0.0.0/16"

    # Subnet 1
    public_subnet_cidr_1: "10.0.1.0/24"
    availability_zone_1: "us-east-1b"

    # Subnet 2
    public_subnet_cidr_2: "10.0.2.0/24"
    availability_zone_2: "us-east-1c"

    # Names / tags
    vpc_name: "k8s-vpc"
    sg_master_name: "k8s-controlplane-sg"
    sg_worker_name: "k8s-workers-sg"
    cluster_name: "dev-{{ aws_region }}-k8s-cluster"

    # SSH / Keypair
    ssh_key_name: "k8s-key"
    ssh_public_key_path: "~/.ssh/k8s_key.pub"

    # Master instance
    master_instance_type: "t3.large"
    master_volume_size: 24
    master_volume_type: "gp3"
    master_count: 1

    # AMI Ubuntu 24.04 en us-east-1
    ami: "ami-0360c520857e3138f"

    # Launch template / user-data: instala podman & tools (simple)
    user_data: |
      #cloud-config
      package_update: true
      package_upgrade: true
      packages:
        - python3
        - python3-pip
        - net-tools
        - podman
        - uidmap
        - slirp4netns
        - fuse-overlayfs
      
      runcmd:
        # Permitir que ubuntu ejecute procesos rootless persistentes
        - loginctl enable-linger ubuntu
        
        # Crear directorio de configuracion rootless
        - sudo -u ubuntu mkdir -p /home/ubuntu/.config/containers
        
        # Crear storage.conf completo con driver overlay y path rootless
        - |
          cat << 'EOF' | sudo tee /home/ubuntu/.config/containers/storage.conf > /dev/null
          [storage]
          driver = "overlay"
          rootless_storage_path = "/var/lib/containers/user/podman/storage"
          
          [storage.options]
          mount_program = "/usr/bin/fuse-overlayfs"
          EOF
        
        # Asignar permisos correctos
        - sudo chown -R ubuntu:ubuntu /home/ubuntu/.config/containers
        
        # Migrar configuracion y verificar entorno rootless
        - sudo -u ubuntu podman system migrate

  gather_facts: false

  vars_files:
    - ../group_vars/all/pass.yml

  tasks:
    - name: Create VPC
      amazon.aws.ec2_vpc_net:
        name: "{{ vpc_name }}"
        cidr_block: "{{ vpc_cidr }}"
        region: "{{ aws_region }}"
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        tags:
          Environment: K8s
      register: vpc

    - name: Create public subnet in first AZ
      amazon.aws.ec2_vpc_subnet:
        vpc_id: "{{ vpc.vpc.id }}"
        cidr: "{{ public_subnet_cidr_1 }}"
        region: "{{ aws_region }}"
        az: "{{ availability_zone_1 }}"
        map_public: true
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        tags:
          Name: k8s-public-subnet-1
      register: subnet_public_1

    - name: Create public subnet in second AZ
      amazon.aws.ec2_vpc_subnet:
        vpc_id: "{{ vpc.vpc.id }}"
        cidr: "{{ public_subnet_cidr_2 }}"
        region: "{{ aws_region }}"
        az: "{{ availability_zone_2 }}"
        map_public: true
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        tags:
          Name: k8s-public-subnet-2
      register: subnet_public_2

    - name: Create Internet Gateway
      amazon.aws.ec2_vpc_igw:
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
      register: igw

    - name: Create Custom Route Table
      amazon.aws.ec2_vpc_route_table:
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        subnets:
          - "{{ subnet_public_1.subnet.id }}"
          - "{{ subnet_public_2.subnet.id }}"
        routes:
          - dest: 0.0.0.0/0
            gateway_id: "{{ igw.gateway_id }}"
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        tags:
          Name: k8s-public-rt
      register: rt

    - name: Detect public IP dynamically
      set_fact:
        my_public_ip: "{{ lookup('url', 'https://checkip.amazonaws.com') | trim }}/32"

    - name: Create Security Group for Kubernetes Control Plane
      amazon.aws.ec2_security_group:
        name: "{{ sg_master_name }}"
        description: "Security Group for Kubernetes Control Plane"
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        rules:
          # SSH solo desde ansible control-node IP
          - proto: tcp
            ports: 22
            cidr_ip: "{{ my_public_ip }}"
            rule_desc: "SSH restricted access"

          # Kubernetes API - accesible desde mi IP
          - proto: tcp
            ports: 6443
            cidr_ip: "{{ my_public_ip }}"
            rule_desc: "K8s API for admin"

        rules_egress:
          - proto: -1
            cidr_ip: 0.0.0.0/0
            rule_desc: "Allow all outbound"
      register: sg_controlplane

    - name: Create Security Group for Kubernetes Worker Nodes
      amazon.aws.ec2_security_group:
        name: "{{ sg_worker_name }}"
        description: "Security Group for Kubernetes Worker Nodes"
        vpc_id: "{{ vpc.vpc.id }}"
        region: "{{ aws_region }}"
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        rules:
          # SSH opcional (podemos quitarlo luego)
          - proto: tcp
            ports: 22
            cidr_ip: "{{ my_public_ip }}"
            rule_desc: "Temporary SSH to workers"

          # NodePort Services (externally accessible)
          - proto: tcp
            ports: "30000-32767"
            cidr_ip: 0.0.0.0/0
            rule_desc: "NodePort TCP services"

          - proto: udp
            ports: "30000-32767"
            cidr_ip: 0.0.0.0/0
            rule_desc: "NodePort UDP services"

        rules_egress:
          - proto: -1
            cidr_ip: 0.0.0.0/0
            rule_desc: "Allow all outbound"
      register: sg_workers

    - name: Add Master inbound rules that depend on Worker SG
      amazon.aws.ec2_security_group:
        name: "{{ sg_master_name }}"
        description: "Security Group for Kubernetes Control Plane"
        region: "{{ aws_region }}"
        purge_rules: false
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        rules:
          # Kubernetes API - accesible desde Workers
          - proto: tcp
            ports: 6443
            group_name: "{{ sg_worker_name }}"
            rule_desc: "K8s API accessible from workers"

          # etcd interno
          - proto: tcp
            ports: "2379-2380"
            group_name: "{{ sg_master_name }}"
            rule_desc: "etcd cluster internal"

          # Kubelet API desde Workers
          - proto: tcp
            ports: 10250
            group_name: "{{ sg_worker_name }}"
            rule_desc: "Kubelet internal from workers"

          # Scheduler / Controller solo internos
          - proto: tcp
            ports: 10257
            group_name: "{{ sg_master_name }}"
            rule_desc: "Controller Manager internal only"

          - proto: tcp
            ports: 10259
            group_name: "{{ sg_master_name }}"
            rule_desc: "Scheduler internal only"

    - name: Add Worker inbound rules from Master SG
      amazon.aws.ec2_security_group:
        name: "{{ sg_worker_name }}"
        description: "Security Group for Kubernetes Worker Nodes"
        region: "{{ aws_region }}"
        purge_rules: false
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        rules:
          # Kubelet API accesible desde control plane
          - proto: tcp
            ports: 10250
            group_name: "{{ sg_master_name }}"
            rule_desc: "Kubelet API from Control Plane"

          # kube-proxy interno
          - proto: tcp
            ports: 10256
            group_name: "{{ sg_master_name }}"
            rule_desc: "kube-proxy internal"

          # Kubernetes API disponible desde workers
          - proto: tcp
            ports: 6443
            group_name: "{{ sg_worker_name }}"
            rule_desc: "API K8s accesible desde workers"

          # Acceso HTTPS para Ingress Controller
          - proto: tcp
            ports: 80
            cidr_ip: 0.0.0.0/0
            rule_desc: "Permitir trafico HTTP publico"

          # Acceso HTTPS para Ingress Controller
          - proto: tcp
            ports: 443
            cidr_ip: 0.0.0.0/0
            rule_desc: "Permitir trafico HTTPS publico"

    - name: Import SSH public key to AWS
      amazon.aws.ec2_key:
        name: "{{ ssh_key_name }}"
        key_material: "{{ lookup('file', ssh_public_key_path) }}"
        region: "{{ aws_region }}"
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
      register: imported_keypair

    - name: Launch master EC2 instance
      amazon.aws.ec2_instance:
        name: "{{ cluster_name }}-master"
        image_id: "{{ ami }}"
        instance_type: "{{ master_instance_type }}"
        key_name: "{{ ssh_key_name }}"
        region: "{{ aws_region }}"
        availability_zone: "{{ availability_zone_1 }}"
        exact_count: "{{ master_count }}"
        tags:
          kubernetes.io/role: control-plane
        filters:
          "tag:kubernetes.io/role": "control-plane"
          instance-state-name: [ "pending", "running", "stopping", "stopped" ]
        aws_access_key: "{{ ec2_access_key }}"
        aws_secret_key: "{{ ec2_secret_key }}"
        network_interfaces:
          - subnet_id: "{{ subnet_public_1.subnet.id }}"
            assign_public_ip: true
            groups:
              - "{{ sg_controlplane.group_id }}"
        volumes:
          - device_name: /dev/sda1
            ebs:
              volume_size: "{{ master_volume_size }}"
              volume_type: "{{ master_volume_type }}"
              delete_on_termination: true
        user_data: "{{ user_data }}"
        wait: true
      register: master_instance

    - name: DEBUG - Show Master public IP and instance_id, Worker IPs
      ansible.builtin.debug:
        msg:
          - "Master Instance ID: {{ master_instance.instances[0].instance_id }}"
          - "Master Public IP: {{ master_instance.instances[0].public_ip_address }}"